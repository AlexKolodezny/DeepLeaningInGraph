# DeepLeaningInGraph
## Текст
### 1 Слайд
Другим подходом к построения нейронных сетей на графах являются графовые свёрточные сети, в них как и в рекурентных нейронных сетях на каждом шаге вектор ембеддинга для вершины пересчитывается как функция он ембеддингов соседних вершин, однако на каждом слое обучаются свои параметры. В результате формула для одного слоя свёрточной сети выглядит так (формула), где Psi перестановочно-инвариантная функция принимающая эмбединги от соседей, а phi и psi маленькие это некоторые параметризованные функции, которые мы обучаем на данном слое.

### 2 Слайд
Примером конкретной такой функции, использованной в Graph Convolution Network, представлена такая ,которую можно получить используя спектральную теорию графов, здесь sigma --- сигмоида, W --- обучаемая матрица, L --- нормированный лапласиан. В качестве перестановочно-инвариантной функции используется сумма.

### 3 Слайд (Sampling)
Одной из проблем обучения нейронных сетей на графах является вычислительная сложность, особенно в тех случаях, когда граф является близким к плотному, в этом случае количество рёбер пропорционально квадрату количества вершин.
Одним из методов борьбы являются методы sampling-а. Например можно для каждой вершины выбирать на каждом слое случайное подмножество соседей, которые будут давать вклад в новый вектор для вершины, такой способ был предложен в архитектуре GraphSAGE. Он позволяет уменьшить вычислительную стоимость в случае плотных графов, но при этом всё равно приходится пересчитывать градиенты для всех вершин.
Другой способ, предложенный в архитектуре FastGCN, это выбрать просто подмножество вершин графа и оптимизировать ембединги только для полученного подграфа. Этот способ особенно эффективный, если вершин очень много, так как нам не надо пересчитывать градиенты для всех вершин.

### 4 Слайд (Pooling)
Другой способ ускорения обучения это добавление слоёв pooling-а, когда кластера вершин по определённой схеме объединяются в одну вершину, в результате чего уменьшаются размеры графа, что в принципе ускоряет обучение.
Pooling так же сам по себе решает задачу кластеризации графа.

### 5 Слайд (Differentiable Pooling)
Одним из вариантов pooling-а является differentiable pooling, который является адаптивным методом, то есть его можно обучить.
В нём для каждой вершины обучается матрица soft-membership попадания в каждый кластер (можно думать что это вероятность), получается она обучением отдельного свёрточного слоя или нескольких свёрточных слоёв и применением функции softmax.
Далее с помощью полученной матрицы надо пересчитать новые эмбединги для вершин (что можно записать через перемножение матриц) и новую матрицу смежности.
Можно заметить, что проблема этого алгоритма это то, что получается полная матрица смежности.

